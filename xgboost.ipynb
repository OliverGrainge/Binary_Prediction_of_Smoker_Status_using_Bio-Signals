{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "import xgboost as xgb\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "data = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. BMI Calculation\n",
    "def feature_engineering(data):\n",
    "    data['BMI'] = data['weight(kg)'] / (data['height(cm)'] / 100) ** 2\n",
    "\n",
    "    # 2. Average Eyesight\n",
    "    data['avg_eyesight'] = (data['eyesight(left)'] + data['eyesight(right)']) / 2\n",
    "\n",
    "    # 3. Average Hearing\n",
    "    data['avg_hearing'] = (data['hearing(left)'] + data['hearing(right)']) / 2\n",
    "\n",
    "    # 4. Total Cholesterol\n",
    "    data['total_cholesterol'] = data['HDL'] + data['LDL']\n",
    "\n",
    "    # 5. AST to ALT ratio\n",
    "    data['AST_to_ALT_ratio'] = data['AST'] / data['ALT']\n",
    "\n",
    "    # Display the first few rows of the dataset with the new features\n",
    "    columns_to_drop = ['id', 'eyesight(left)', 'eyesight(right)', 'hearing(left)', 'hearing(right)', 'HDL', 'LDL', 'smoking']\n",
    "    data = data.drop(columns=columns_to_drop)\n",
    "    return data \n",
    "\n",
    "X_features = feature_engineering(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_columns = ['age', 'height(cm)', 'weight(kg)', 'waist(cm)', 'systolic',\n",
    "                      'hemoglobin', 'serum creatinine', 'AST', 'ALT', 'Gtp', \n",
    "                      'BMI', 'avg_eyesight', 'avg_hearing', 'total_cholesterol', 'AST_to_ALT_ratio', \"fasting blood sugar\", 'Cholesterol', 'triglyceride', 'relaxation']\n",
    "\n",
    "categorical_columns = [col for col in X_features.columns if col not in continuous_columns]\n",
    "\n",
    "# Adjust the column transformer to handle unknown categories\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), continuous_columns),\n",
    "        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_columns)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((127404, 21), (31852, 21))"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Drop redundant columns\n",
    "columns_to_drop = ['id', 'eyesight(left)', 'eyesight(right)', 'hearing(left)', 'hearing(right)', 'HDL', 'LDL']\n",
    "data = data.drop(columns=columns_to_drop)\n",
    "\n",
    "# Split the data into training and validation sets (80% train, 20% validation)\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Separate features and target variable for both training and validation sets\n",
    "X_train = train_data[\"smoking\"]\n",
    "y_train = train_data['smoking']\n",
    "\n",
    "X_val = val_data.drop(columns='smoking')\n",
    "y_val = val_data['smoking']\n",
    "\n",
    "X_train.shape, X_val.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['age',\n",
       "  'height(cm)',\n",
       "  'weight(kg)',\n",
       "  'waist(cm)',\n",
       "  'systolic',\n",
       "  'hemoglobin',\n",
       "  'serum creatinine',\n",
       "  'AST',\n",
       "  'ALT',\n",
       "  'Gtp',\n",
       "  'BMI',\n",
       "  'avg_eyesight',\n",
       "  'avg_hearing',\n",
       "  'total_cholesterol',\n",
       "  'AST_to_ALT_ratio',\n",
       "  'fasting blood sugar',\n",
       "  'Cholesterol',\n",
       "  'triglyceride',\n",
       "  'relaxation'],\n",
       " ['Urine protein', 'dental caries'])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identifying continuous and categorical columns\n",
    "continuous_columns = ['age', 'height(cm)', 'weight(kg)', 'waist(cm)', 'systolic',\n",
    "                      'hemoglobin', 'serum creatinine', 'AST', 'ALT', 'Gtp', \n",
    "                      'BMI', 'avg_eyesight', 'avg_hearing', 'total_cholesterol', 'AST_to_ALT_ratio', \"fasting blood sugar\", 'Cholesterol', 'triglyceride', 'relaxation']\n",
    "\n",
    "categorical_columns = [col for col in X_train.columns if col not in continuous_columns]\n",
    "\n",
    "continuous_columns, categorical_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((127404, 25), (31852, 25))"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjust the column transformer to handle unknown categories\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), continuous_columns),\n",
    "        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_columns)\n",
    "    ])\n",
    "\n",
    "# Fit the transformer on the training data and transform the training and validation data\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_val_transformed = preprocessor.transform(X_val)\n",
    "\n",
    "X_train_transformed.shape, X_val_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_folds = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(\"smoking\", axis=1).to_numpy()\n",
    "y = data[\"smoking\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8613910973002025\n"
     ]
    }
   ],
   "source": [
    "model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\n",
    "model.fit(X_train_transformed, y_train)\n",
    "preds = model.predict_proba(X_val_transformed)[:, 1]\n",
    "print(roc_auc_score(y_val, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.863735551103294 1.9040727217446967e-05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scores = cross_val_score(model, Xtrai, y, cv=K_folds, scoring=\"roc_auc\")\n",
    "print(np.mean(scores), np.var(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets do a bayesian hyperparmeter search over xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    'learning_rate': hp.quniform('learning_rate', 0.01, 0.5, 0.01),\n",
    "    'max_depth': hp.choice('max_depth', range(1, 15, 1)),\n",
    "    'n_estimators': hp.choice('n_estimators', range(20, 205, 5)),\n",
    "    'gamma': hp.quniform('gamma', 0, 0.5, 0.01),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    'subsample': hp.quniform('subsample', 0.1, 1, 0.01),\n",
    "    'colsample_bytree': hp.quniform('colsample_bytree', 0.1, 1.0, 0.01)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    scores = cross_val_score(model, X, y, cv=K_folds, scoring=\"roc_auc\")\n",
    "    loss = 1 - np.mean(scores)\n",
    "    return {'loss': loss, 'status': STATUS_OK}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:21<00:00, 14.11s/trial, best loss: 0.13433569053900718]\n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=10,  # Number of iterations\n",
    "            trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(**best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.76, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=0.21, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.13, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
       "              min_child_weight=8.0, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=34, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.76, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=0.21, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.13, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
       "              min_child_weight=8.0, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=34, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.76, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=0.21, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.13, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
       "              min_child_weight=8.0, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=34, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8569495120946011"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict_proba(X)[:, 1].flatten()\n",
    "roc_auc_score(y, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106171\n",
      "106171\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"data/test.csv\")\n",
    "print(len(test_data))\n",
    "ids = test_data[\"id\"].to_numpy()\n",
    "print(len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Feature shape mismatch, expected: 28, got 23",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[143], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m preds \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict_proba(test_data\u001b[39m.\u001b[39;49mto_numpy())[:, \u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(preds))\n",
      "File \u001b[0;32m~/miniconda3/envs/visualloc_env/lib/python3.10/site-packages/xgboost/sklearn.py:1628\u001b[0m, in \u001b[0;36mXGBClassifier.predict_proba\u001b[0;34m(self, X, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1626\u001b[0m     class_prob \u001b[39m=\u001b[39m softmax(raw_predt, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m   1627\u001b[0m     \u001b[39mreturn\u001b[39;00m class_prob\n\u001b[0;32m-> 1628\u001b[0m class_probs \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mpredict(\n\u001b[1;32m   1629\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m   1630\u001b[0m     validate_features\u001b[39m=\u001b[39;49mvalidate_features,\n\u001b[1;32m   1631\u001b[0m     base_margin\u001b[39m=\u001b[39;49mbase_margin,\n\u001b[1;32m   1632\u001b[0m     iteration_range\u001b[39m=\u001b[39;49miteration_range,\n\u001b[1;32m   1633\u001b[0m )\n\u001b[1;32m   1634\u001b[0m \u001b[39mreturn\u001b[39;00m _cls_predict_proba(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_, class_probs, np\u001b[39m.\u001b[39mvstack)\n",
      "File \u001b[0;32m~/miniconda3/envs/visualloc_env/lib/python3.10/site-packages/xgboost/sklearn.py:1164\u001b[0m, in \u001b[0;36mXGBModel.predict\u001b[0;34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_can_use_inplace_predict():\n\u001b[1;32m   1163\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1164\u001b[0m         predts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_booster()\u001b[39m.\u001b[39;49minplace_predict(\n\u001b[1;32m   1165\u001b[0m             data\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m   1166\u001b[0m             iteration_range\u001b[39m=\u001b[39;49miteration_range,\n\u001b[1;32m   1167\u001b[0m             predict_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmargin\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39mif\u001b[39;49;00m output_margin \u001b[39melse\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39mvalue\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1168\u001b[0m             missing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmissing,\n\u001b[1;32m   1169\u001b[0m             base_margin\u001b[39m=\u001b[39;49mbase_margin,\n\u001b[1;32m   1170\u001b[0m             validate_features\u001b[39m=\u001b[39;49mvalidate_features,\n\u001b[1;32m   1171\u001b[0m         )\n\u001b[1;32m   1172\u001b[0m         \u001b[39mif\u001b[39;00m _is_cupy_array(predts):\n\u001b[1;32m   1173\u001b[0m             \u001b[39mimport\u001b[39;00m \u001b[39mcupy\u001b[39;00m  \u001b[39m# pylint: disable=import-error\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/visualloc_env/lib/python3.10/site-packages/xgboost/core.py:2427\u001b[0m, in \u001b[0;36mBooster.inplace_predict\u001b[0;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[1;32m   2423\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m   2424\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`shape` attribute is required when `validate_features` is True.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2425\u001b[0m         )\n\u001b[1;32m   2426\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data\u001b[39m.\u001b[39mshape) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_features() \u001b[39m!=\u001b[39m data\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:\n\u001b[0;32m-> 2427\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2428\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFeature shape mismatch, expected: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_features()\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2429\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgot \u001b[39m\u001b[39m{\u001b[39;00mdata\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2430\u001b[0m         )\n\u001b[1;32m   2432\u001b[0m \u001b[39mif\u001b[39;00m _is_np_array_like(data):\n\u001b[1;32m   2433\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m _ensure_np_dtype\n",
      "\u001b[0;31mValueError\u001b[0m: Feature shape mismatch, expected: 28, got 23"
     ]
    }
   ],
   "source": [
    "preds = model.predict_proba(test_data.to_numpy())[:, 1].flatten()\n",
    "print(len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id   smoking\n",
      "0  159256  0.608602\n",
      "1  159257  0.222500\n",
      "2  159258  0.691817\n",
      "3  159259  0.070218\n",
      "4  159260  0.500467\n"
     ]
    }
   ],
   "source": [
    "predictions = {\"id\":ids, \"smoking\":preds}\n",
    "test_df = pd.DataFrame.from_dict(predictions)\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(\"test_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('visualloc_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "700466eb92602a8efd37f4d86ea7408737fa7bc83b433f2b773fdb7c0b56206f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
